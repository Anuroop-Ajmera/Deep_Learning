{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a CNN classifier to classify the pathology images as foreground vs background. The CNN should follow this architecture: CONV layer with 16 3x3 filters with pad 1 stride 1, RELU, POOL 2x2 with stride 2, CONV layer with 8 3x3 filters with pad 1 stride 1, RELU, POOL 2x2 with stride 2, Dense layer of size 64, RELU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as nnF\n",
    "\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchviz import make_dot\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder('./Dataset/pathologyData/cancerData',\n",
    "                 transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the mean and standard deviation of images\n",
    "loader = DataLoader(dataset,\n",
    "                         batch_size=10,\n",
    "                         num_workers=0,\n",
    "                         shuffle=False)\n",
    "mean = 0.\n",
    "stdev = 0.\n",
    "for images, _ in loader:\n",
    "    batch_samples = images.size(0)\n",
    "    images = images.view(batch_samples, images.size(1), -1)\n",
    "    mean += images.mean(2).sum(0)\n",
    "    stdev += images.std(2).sum(0)\n",
    "\n",
    "mean /= len(loader.dataset)\n",
    "stdev /= len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8282, 0.8652, 0.8203])\n",
      "tensor([0.1153, 0.0852, 0.0854])\n"
     ]
    }
   ],
   "source": [
    "print(mean)\n",
    "print(stdev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_transformations = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.8282, 0.8652, 0.8203], std=[0.1153, 0.0852, 0.0854])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder(\"./Dataset/pathologyData/cancerData\", transform = my_transformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train, dataset_test = torch.utils.data.random_split(dataset, [int(len(dataset) * 0.75), int(len(dataset)*0.25)], generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put into a Dataloader using torch library\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=30, shuffle=True)\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=30, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_Net(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc1): Linear(in_features=5000, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class CNN_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_Net, self).__init__()\n",
    "        # nn.Conv2d API : torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1,padding=1)  # 16 filters of 3x3 size with pad =1 and stride = 1\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3,stride=1,padding=1) # 8 filters of 3x3 size with pad =1 and stride = 1\n",
    "        # nn.Linear API : torch.nn.Linear(in_features, out_features, bias=True)\n",
    "        self.fc1 = nn.Linear(25*25*8, 64)\n",
    "        self.fc2 = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nnF.max_pool2d(nnF.relu(self.conv1(x)), 2)\n",
    "        x = nnF.max_pool2d(nnF.relu(self.conv2(x)), 2)\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = nnF.relu(self.fc1(x))\n",
    "        x = nnF.log_softmax(self.fc2(x), dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = CNN_Net()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 3, 100, 100]) torch.Size([30]) torch.Size([30, 2])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# get a random training batch\n",
    "iterator = iter(dataloader_train)\n",
    "X_batch, y_batch = next(iterator)\n",
    "print(X_batch.shape, y_batch.shape , model(X_batch).shape)\n",
    "\n",
    "# pass a batch through the model and visualize the architecture\n",
    "# NOTE: we do not have to explicitly call model.forward(inputs), instead we just do model(inputs)\n",
    "# This is because PyTorch internally takes care of, giving us this syntactic sugar\n",
    "\n",
    "#make_dot(model(X_batch), params=dict(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, device, data_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    loss_train = 0\n",
    "    num_correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(data_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_train += loss.item()\n",
    "        prediction = output.argmax(dim=1)\n",
    "        num_correct += prediction.eq(target).sum().item()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.3f}%)]\\tLoss: {:.3f}\\tAccuracy: {:.3f}%'.format(\n",
    "                epoch+1, batch_idx * len(data), len(data_loader.dataset),\n",
    "                100. * batch_idx / len(data_loader), loss_train / (batch_idx + 1),\n",
    "                100. * num_correct / (len(data) * (batch_idx + 1))))\n",
    "    loss_train /= len(data_loader)\n",
    "    accuracy = num_correct / len(data_loader.dataset)\n",
    "    return loss_train, accuracy\n",
    "    \n",
    "\n",
    "def testing(model, device, data_loader, criterion):\n",
    "    model.eval()\n",
    "    loss_test = 0\n",
    "    num_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss_test += loss.item()  # sum up batch loss\n",
    "            prediction = output.argmax(dim=1)\n",
    "            num_correct += prediction.eq(target).sum().item()\n",
    "    loss_test /= len(data_loader)\n",
    "    accuracy = num_correct / len(data_loader.dataset)\n",
    "    return loss_test, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu' if not torch.cuda.is_available() else 'cuda')\n",
    "model = CNN_Net().to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/2250 (0.000%)]\tLoss: 0.673\tAccuracy: 43.333%\n",
      "Train Epoch: 1 [300/2250 (13.333%)]\tLoss: 0.532\tAccuracy: 67.273%\n",
      "Train Epoch: 1 [600/2250 (26.667%)]\tLoss: 0.457\tAccuracy: 76.349%\n",
      "Train Epoch: 1 [900/2250 (40.000%)]\tLoss: 0.378\tAccuracy: 83.333%\n",
      "Train Epoch: 1 [1200/2250 (53.333%)]\tLoss: 0.297\tAccuracy: 87.317%\n",
      "Train Epoch: 1 [1500/2250 (66.667%)]\tLoss: 0.243\tAccuracy: 89.739%\n",
      "Train Epoch: 1 [1800/2250 (80.000%)]\tLoss: 0.210\tAccuracy: 91.311%\n",
      "Train Epoch: 1 [2100/2250 (93.333%)]\tLoss: 0.181\tAccuracy: 92.488%\n",
      "Epoch 1 Train: Loss: 0.171, Accuracy: 92.889%\n",
      "\n",
      "Epoch 1 Test : Loss: 0.006, Accuracy: 99.867%\n",
      "\n",
      "Train Epoch: 2 [0/2250 (0.000%)]\tLoss: 0.004\tAccuracy: 100.000%\n",
      "Train Epoch: 2 [300/2250 (13.333%)]\tLoss: 0.001\tAccuracy: 100.000%\n",
      "Train Epoch: 2 [600/2250 (26.667%)]\tLoss: 0.004\tAccuracy: 99.841%\n",
      "Train Epoch: 2 [900/2250 (40.000%)]\tLoss: 0.007\tAccuracy: 99.677%\n",
      "Train Epoch: 2 [1200/2250 (53.333%)]\tLoss: 0.005\tAccuracy: 99.756%\n",
      "Train Epoch: 2 [1500/2250 (66.667%)]\tLoss: 0.007\tAccuracy: 99.739%\n",
      "Train Epoch: 2 [1800/2250 (80.000%)]\tLoss: 0.006\tAccuracy: 99.781%\n",
      "Train Epoch: 2 [2100/2250 (93.333%)]\tLoss: 0.005\tAccuracy: 99.812%\n",
      "Epoch 2 Train: Loss: 0.005, Accuracy: 99.822%\n",
      "\n",
      "Epoch 2 Test : Loss: 0.003, Accuracy: 99.867%\n",
      "\n",
      "Train Epoch: 3 [0/2250 (0.000%)]\tLoss: 0.000\tAccuracy: 100.000%\n",
      "Train Epoch: 3 [300/2250 (13.333%)]\tLoss: 0.000\tAccuracy: 100.000%\n",
      "Train Epoch: 3 [600/2250 (26.667%)]\tLoss: 0.000\tAccuracy: 100.000%\n",
      "Train Epoch: 3 [900/2250 (40.000%)]\tLoss: 0.000\tAccuracy: 100.000%\n",
      "Train Epoch: 3 [1200/2250 (53.333%)]\tLoss: 0.000\tAccuracy: 100.000%\n",
      "Train Epoch: 3 [1500/2250 (66.667%)]\tLoss: 0.001\tAccuracy: 100.000%\n",
      "Train Epoch: 3 [1800/2250 (80.000%)]\tLoss: 0.001\tAccuracy: 100.000%\n",
      "Train Epoch: 3 [2100/2250 (93.333%)]\tLoss: 0.001\tAccuracy: 100.000%\n",
      "Epoch 3 Train: Loss: 0.000, Accuracy: 100.000%\n",
      "\n",
      "Epoch 3 Test : Loss: 0.006, Accuracy: 99.867%\n",
      "\n",
      "Train Epoch: 4 [0/2250 (0.000%)]\tLoss: 0.000\tAccuracy: 100.000%\n",
      "Train Epoch: 4 [300/2250 (13.333%)]\tLoss: 0.001\tAccuracy: 100.000%\n",
      "Train Epoch: 4 [600/2250 (26.667%)]\tLoss: 0.000\tAccuracy: 100.000%\n",
      "Train Epoch: 4 [900/2250 (40.000%)]\tLoss: 0.000\tAccuracy: 100.000%\n",
      "Train Epoch: 4 [1200/2250 (53.333%)]\tLoss: 0.000\tAccuracy: 100.000%\n",
      "Train Epoch: 4 [1500/2250 (66.667%)]\tLoss: 0.000\tAccuracy: 100.000%\n",
      "Train Epoch: 4 [1800/2250 (80.000%)]\tLoss: 0.000\tAccuracy: 100.000%\n",
      "Train Epoch: 4 [2100/2250 (93.333%)]\tLoss: 0.000\tAccuracy: 100.000%\n",
      "Epoch 4 Train: Loss: 0.000, Accuracy: 100.000%\n",
      "\n",
      "Epoch 4 Test : Loss: 0.005, Accuracy: 99.867%\n",
      "\n",
      "Train Epoch: 5 [0/2250 (0.000%)]\tLoss: 0.000\tAccuracy: 100.000%\n",
      "Train Epoch: 5 [300/2250 (13.333%)]\tLoss: 0.000\tAccuracy: 100.000%\n",
      "Train Epoch: 5 [600/2250 (26.667%)]\tLoss: 0.001\tAccuracy: 100.000%\n",
      "Train Epoch: 5 [900/2250 (40.000%)]\tLoss: 0.000\tAccuracy: 100.000%\n",
      "Train Epoch: 5 [1200/2250 (53.333%)]\tLoss: 0.000\tAccuracy: 100.000%\n",
      "Train Epoch: 5 [1500/2250 (66.667%)]\tLoss: 0.000\tAccuracy: 100.000%\n",
      "Train Epoch: 5 [1800/2250 (80.000%)]\tLoss: 0.000\tAccuracy: 100.000%\n",
      "Train Epoch: 5 [2100/2250 (93.333%)]\tLoss: 0.000\tAccuracy: 100.000%\n",
      "Epoch 5 Train: Loss: 0.000, Accuracy: 100.000%\n",
      "\n",
      "Epoch 5 Test : Loss: 0.005, Accuracy: 99.867%\n",
      "\n",
      "Train Epoch: 6 [0/2250 (0.000%)]\tLoss: 0.000\tAccuracy: 100.000%\n",
      "Train Epoch: 6 [300/2250 (13.333%)]\tLoss: 0.000\tAccuracy: 100.000%\n",
      "Train Epoch: 6 [600/2250 (26.667%)]\tLoss: 0.000\tAccuracy: 100.000%\n",
      "Train Epoch: 6 [900/2250 (40.000%)]\tLoss: 0.000\tAccuracy: 100.000%\n",
      "Train Epoch: 6 [1200/2250 (53.333%)]\tLoss: 0.000\tAccuracy: 100.000%\n",
      "Train Epoch: 6 [1500/2250 (66.667%)]\tLoss: 0.000\tAccuracy: 100.000%\n",
      "Train Epoch: 6 [1800/2250 (80.000%)]\tLoss: 0.000\tAccuracy: 100.000%\n",
      "Train Epoch: 6 [2100/2250 (93.333%)]\tLoss: 0.000\tAccuracy: 100.000%\n",
      "Epoch 6 Train: Loss: 0.000, Accuracy: 100.000%\n",
      "\n",
      "Epoch 6 Test : Loss: 0.006, Accuracy: 99.867%\n",
      "\n",
      "Train Epoch: 7 [0/2250 (0.000%)]\tLoss: 0.000\tAccuracy: 100.000%\n",
      "Train Epoch: 7 [300/2250 (13.333%)]\tLoss: 0.000\tAccuracy: 100.000%\n",
      "Train Epoch: 7 [600/2250 (26.667%)]\tLoss: 0.000\tAccuracy: 100.000%\n",
      "Train Epoch: 7 [900/2250 (40.000%)]\tLoss: 0.000\tAccuracy: 100.000%\n",
      "Train Epoch: 7 [1200/2250 (53.333%)]\tLoss: 0.000\tAccuracy: 100.000%\n",
      "Train Epoch: 7 [1500/2250 (66.667%)]\tLoss: 0.000\tAccuracy: 100.000%\n",
      "Train Epoch: 7 [1800/2250 (80.000%)]\tLoss: 0.000\tAccuracy: 100.000%\n",
      "Train Epoch: 7 [2100/2250 (93.333%)]\tLoss: 0.000\tAccuracy: 100.000%\n",
      "Epoch 7 Train: Loss: 0.000, Accuracy: 100.000%\n",
      "\n",
      "Epoch 7 Test : Loss: 0.006, Accuracy: 99.867%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(7):\n",
    "    loss_train, acc_train = training(model, device, dataloader_train, optimizer, criterion, epoch)\n",
    "    print('Epoch {} Train: Loss: {:.3f}, Accuracy: {:.3f}%\\n'.format(\n",
    "        epoch+1, loss_train, 100. * acc_train))\n",
    "    loss_test, acc_test = testing(model, device, dataloader_test, criterion)\n",
    "    print('Epoch {} Test : Loss: {:.3f}, Accuracy: {:.3f}%\\n'.format(\n",
    "        epoch+1, loss_test, 100. * acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
